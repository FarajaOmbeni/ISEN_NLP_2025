{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Travaux Pratiques : Construire un RAG sur un seul document avec Google Colab"
      ],
      "metadata": {
        "id": "n8htu0WWW9gB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectif\n",
        "\n",
        "L'objectif de ce TP est d'apprendre à mettre en place un système de RAG (Retrieval-Augmented Generation) à partir d'un seul document en utilisant Google Colab. Nous allons :\n",
        "\n",
        "1. Charger un document texte.\n",
        "\n",
        "2. Extraire et indexer son contenu.\n",
        "\n",
        "3. Implémenter une recherche de passage pertinente.\n",
        "\n",
        "4. Utiliser un modèle de génération pour répondre à des questions sur le document."
      ],
      "metadata": {
        "id": "bsxxBTIcW_aY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installer les packages"
      ],
      "metadata": {
        "id": "d04j7q9pXxPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb langchain-community langchain-google-genai"
      ],
      "metadata": {
        "id": "rFT65kGyXzpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configurer l'API de Google AI Studio"
      ],
      "metadata": {
        "id": "oa6Ed5VcYuMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons configurer l'API Gemini de Google AI Studio pour intéragir avec un modèle de génération."
      ],
      "metadata": {
        "id": "H4ZIS58jYzbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"VOTRE_CLEF\""
      ],
      "metadata": {
        "id": "Y1384Jjzb4Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Charger un document texte"
      ],
      "metadata": {
        "id": "kTJfMMdZXHfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons utiliser un fichier texte comme source d'information. Vous pouvez soit en uploader un, soit utiliser un texte d'exemple."
      ],
      "metadata": {
        "id": "voDND6REXLWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()  # Permet d'uploader un fichier texte\n",
        "\n",
        "# Lire le contenu du fichier\n",
        "file_name = list(uploaded.keys())[0]\n",
        "with open(file_name, 'r', encoding='utf-8') as file:\n",
        "    document_text = file.read()\n",
        "\n",
        "print(\"Aperçu du document:\", document_text[:500])  # Affiche un extrait du document"
      ],
      "metadata": {
        "id": "ucT2D7JdW8CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Segmenter et indexer le document"
      ],
      "metadata": {
        "id": "QdD5R2n7XbsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons découper le document en morceaux pour faciliter la recherche."
      ],
      "metadata": {
        "id": "8Z0IgWPsXhDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilisez la fonction \"RecursiveCharacterTextSplitter\" de lanchain pour découper le document avec une taille de chunk de 500 et un overlap de 50."
      ],
      "metadata": {
        "id": "HNC9dLnFej6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Découpage du document en morceaux de 500 caractères avec un chevauchement\n",
        "text_splitter = None\n",
        "\n",
        "# Appliquez text_splitter sur notre document\n",
        "documents = None\n",
        "print(f\"Nombre de segments créés : {len(documents)}\")"
      ],
      "metadata": {
        "id": "2TBqKRFTW7-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Créer une base de données vectorielle\n",
        "\n",
        "Nous allons utiliser ChromaDB pour stocker et interroger notre document."
      ],
      "metadata": {
        "id": "WM43cmiHW7f8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO3bxpSuW4oq"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# Initialiser l'index vectoriel avec Chroma\n",
        "vectorstore = None\n",
        "\n",
        "# Ajouter les documents indexés ne utilisant la méthode 'add_texts' de vectorstore\n",
        "None\n",
        "\n",
        "print(\"Indexation terminée !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Rechercher des passages pertinents"
      ],
      "metadata": {
        "id": "OtYKb62TcvS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons interroger notre base pour retrouver les passages les plus pertinents."
      ],
      "metadata": {
        "id": "E04H9uIbcyya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Quel est le sujet principal du document ?\"\n",
        "\n",
        "# Utilisez la méthode 'similarity_search' de vectorstore pour trouver les 3 chunks les plus proche de la question\n",
        "retrieved_docs = None\n",
        "\n",
        "print(\"Passages retrouvés :\")\n",
        "for doc in retrieved_docs:\n",
        "    print(\"-\", doc.page_content)"
      ],
      "metadata": {
        "id": "AJAYFJWqXuug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Pour quand est prévu la sortie de la beta ?\"\n",
        "\n",
        "# Utilisez la méthode 'similarity_search' de vectorstore pour trouver les 3 chunks les plus proche de la question\n",
        "retrieved_docs = None\n",
        "\n",
        "print(\"Passages retrouvés :\")\n",
        "for doc in retrieved_docs:\n",
        "    print(\"-\", doc.page_content)"
      ],
      "metadata": {
        "id": "HRDVyULVc8hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Générer une réponse avec un LLM"
      ],
      "metadata": {
        "id": "sDkPrHCadIeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous utilisons l'API Gemini de Google AI Studio pour génrer une réponse basée sur les passages récupéres."
      ],
      "metadata": {
        "id": "1E_7Kpp4dMDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "async def generate_response(prompt):\n",
        "\n",
        "    # Initialisez le modèle \"gemini-pro\" en utilisant la fonction GenerativeModel\n",
        "    model = None\n",
        "\n",
        "    # Utilisez la fonction 'generate_content' pour générer la réponse de 'prompt'\n",
        "    response = None\n",
        "\n",
        "    return response.text.strip()"
      ],
      "metadata": {
        "id": "Z_uf4CgxdE2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1"
      ],
      "metadata": {
        "id": "HXBDPlAsfoCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Pour quand est prévu la sortie de la beta ?\"\n",
        "\n",
        "# Utilisez la méthode 'similarity_search' de vectorstore pour trouver les 3 chunks les plus proche de la question\n",
        "retrieved_docs = None\n",
        "\n",
        "print(\"Passages retrouvés :\")\n",
        "for doc in retrieved_docs:\n",
        "    print(\"-\", doc.page_content)"
      ],
      "metadata": {
        "id": "KC_LM5qsflbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construire le prompt basé sur les passages retrouvés\n",
        "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "# Ecrivez votre prompt pour ordonner la recherche en fonction de ce qui a été trouvé dans 'context' par rapport à ce que veut l'utilisateur dans 'query'\n",
        "prompt = None\n",
        "\n",
        "# Obtenir la réponse en utilisant la fonction 'generate_response'\n",
        "response = None\n",
        "print(\"Réponse générée :\", response)"
      ],
      "metadata": {
        "id": "JSGzbBTPdS9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2"
      ],
      "metadata": {
        "id": "BaAgKZocfpkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Quel est le sujet principal du document ?\"\n",
        "\n",
        "# Utilisez la méthode 'similarity_search' de vectorstore pour trouver les 3 chunks les plus proche de la question\n",
        "retrieved_docs = None\n",
        "\n",
        "print(\"Passages retrouvés :\")\n",
        "for doc in retrieved_docs:\n",
        "    print(\"-\", doc.page_content)"
      ],
      "metadata": {
        "id": "p_9wuz9lfm3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construire le prompt basé sur les passages retrouvés\n",
        "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "# Ecrivez votre prompt pour ordonner la recherche en fonction de ce qui a été trouvé dans 'context' par rapport à ce que veut l'utilisateur dans 'query'\n",
        "prompt = None\n",
        "\n",
        "# Obtenir la réponse en utilisant la fonction 'generate_response'\n",
        "response = None\n",
        "print(\"Réponse générée :\", response)"
      ],
      "metadata": {
        "id": "OND0xL51fnSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3"
      ],
      "metadata": {
        "id": "eGNKLj3NfrHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Y a t'il des anomalies dans le document ?\"\n",
        "\n",
        "# Utilisez la méthode 'similarity_search' de vectorstore pour trouver les 3 chunks les plus proche de la question\n",
        "retrieved_docs = None\n",
        "\n",
        "print(\"Passages retrouvés :\")\n",
        "for doc in retrieved_docs:\n",
        "    print(\"-\", doc.page_content)"
      ],
      "metadata": {
        "id": "GtI6Wx8geCJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construire le prompt basé sur les passages retrouvés\n",
        "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "# Ecrivez votre prompt pour ordonner la recherche en fonction de ce qui a été trouvé dans 'context' par rapport à ce que veut l'utilisateur dans 'query'\n",
        "prompt = None\n",
        "\n",
        "# Obtenir la réponse en utilisant la fonction 'generate_response'\n",
        "response = None\n",
        "print(\"Réponse générée :\", response)"
      ],
      "metadata": {
        "id": "tgqMqUvkeG9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2YSened_eK3r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}